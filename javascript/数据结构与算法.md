## 1. 概述

程序是由于数据结构和算法组成的。好的算法可以让代码化繁为简，也可以提升代码运行效率，所以目前很多公司的面试都会考虑算法。首先要搞懂什么是数据结构，什么是算法。

从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。 图书馆储藏书籍你肯定⻅过吧?为了方便查找，图书管理员一般会将书籍分⻔别类进行“存储”。按照一定规律编号，就是书籍这种“数据”的存储结构。

那如何来查找一本书呢?有很多种办法，你当然可以一本一本地找，也可以先根据书籍类别的编号，是人文，还是科学、 计算机，来定位书架，然后再依次查找。笼统地说，这些查找方法都是算法。

从狭义上讲，某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些都 是前人智慧的结晶，可以直接拿来用。经典数据结构和算法，都是前人从很多实际操作场景中抽象出来 的，经过非常多的求证和检验，可以高效地解决很多实际的开发问题。
 
数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。

比如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。

想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。

## 2. 复杂度

数据结构和算法本身解决的是```快```和```省```的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行效率是算法一个非常重要的考量指标。如何衡量编写的算法代码的执行效率。只要讲到数据结构与算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空 间复杂度分析呢?这种分析方法能比我实实在在跑一遍得到的数据更准确吗?

首先，可以肯定地说，这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。

测试环境中硬件的不同会对测试结果有很大的影响。比如同样一段代码，分别用```Intel Core i9```处理器和```Intel Core i3```处 理器来运行，不用说，```i9```处理器要比```i3```处理器执行的速度快很多。还有，比如原本在这台机器上```a```代码执行的速度比```b```代码要快，等换到另一台机器上时，可能会有截然相反的结果。

对同一个排序算法，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快!

算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用肉眼得到一段代码的执行时间，比如这里有段非常简单的代码，求```1，2，3...n```的累加和。现在，我就带你一块来估算一下这段代码的执行时间。

```js
function cal(n) {
  var sum = 0;
  var i = 1;
  for (; i <= n; i++) {
    sum = sum + i;
  }
  return sum; 
}
```

从```CPU```的⻆度来看，这段代码的每一行都执行着类似的操作: 读数据-运算-写数据。尽管每行代码对应的```CPU```执行的个数、执行的时间都不一样，但是这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为```unit_time```，在这个假设的基础之上，评估这段代码的总执行时间。

第``2、3``行代码分别需要``1``个```unit_time```的执行时间，第```4、5```行都运行了```n```遍，所以需要```2n * unit_time```的执行时间，所以这段代码总 的执行时间就是```(2n + 2) * unit_time```。可以看出来，所有代码的执行时间```T(n)```与每行代码的执行次数成正比。

```js
function cal(n) {
  var sum = 0;
  var i = 1;
  var j = 1;
  for (; i <= n; i++) { 
    j = 1;
    for (; j <= n; j++) { 
      sum = sum + i * j;
    }
  }
}
```

第```2、3、4```行代码，每行都需要1个```unit_time```的执行时间，第```5、6```行代码循环执行了```n```遍，需要```2n * unit_time```的执行时间，第```7、8```行代码循环执行了```n2```遍，所以需要```2n2 * unit_time```的执行时间。所以，整段代码总的执行时间```T(n) = (2n2 + 2n + 3 ) * unit_time```。

尽管不知道```unit_time```的具体值，但是通过这两段代码执行时间的推导过程，可以得到一个非常重要的规律，所有代码的执行时间```T(n)```与每行代码的执行次数n成正比。

```js
T(n) = O(f(n))
```

```T(n)```表示代码执行的时间，```n```表示数据规模的大小，```f(n)```表示每行代码执行的次数总和。因为这是一个公式，所以用```f(n)```来表示。公式中的```O```，表示代码的执行时间```T(n)```与```f(n)```表达式成正比。

所以，第一个例子中的```T(n) = O(2n + 2)```，第二个例子中的T```(n) = O(2n + 2n + 3)```。这就是大```O```时间复杂度表示法。大```O```时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增⻓的变化趋势，所以，也叫作渐进时间复杂度(```asymptotic time complexity```)，简称时间复杂度。

当n很大时，可以把它想象成```10000```、```100000```。而公式中的低阶、常量、系数三部分并不左右增⻓趋势，所以都可以忽略。 只需要记录一个最大量级就可以了，如果用大```O```表示法表示刚讲的那两段代码的时间复杂度，就可以记为```T(n) = O(n)```和```T(n) = O(n2)```。

## 3. 时间复杂度

只关注循环执行次数最多的一段代码.

大```O```这种复杂度表示方法只是表示一种变化趋势。通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。这段核心代码执行次数的```n```的量级，就是整段要分析代码的时间复杂度。

```js
function cal(n) {
  var sum = 0;
  var i = 1;
  for (; i <= n; i++) {
    sum = sum + i;
  }
  return sum; 
}
```

第```2、3```行代码都是常量级的执行时间，与```n```的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第```4、5```行代码，所以这块代码要重点分析，这两行代码被执行了```n```次，所以总的时间复杂度就是```O(n)```。

总复杂度等于量级最大的那段代码的复杂度。

```js
function cal(n) {
  var sum_1 = 0;
  var p = 1;
  for (; p < 100; p++) {
      sum_1 = sum_1 + p;
  }
  var sum_2 = 0;
  var q = 1;
  for (; q < n; q++) {
      sum_2 = sum_2 + q;
  }
  var sum_3 = 0;
  var i = 1;
  var j = 1;
  for (; i <= n; i++) {
    j = 1;
    for (; j <= n; j++) {
      sum_3 = sum_3 + i * j; 
    }
  }
  return sum_1 + sum_2 + sum_3; 
}
```

可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。

第一段代码循环执行了```100```次，是一个常量的执行时间，跟n的规模无关，即便这段代码循环```10000```次、```100000```次，只要是一个已知的数，跟```n```无关，照样也是常量级的执行时间。因为当```n```无限大的时候，任何常数都可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增⻓的变化趋势，所以不管常量的执行时间多大，都可以忽略掉。因为它本身对增⻓趋势并没有影响。

那第二段代码和第三段代码的时间复杂度是是```O(n)```和```O(n2)```，取其中最大的量级。所以，整段代码的时间复杂度就为```O(n2)```。

总的时间复杂度等于量级最大的那段代码的时间复杂度。如果```T1(n) = O(f(n))```，```T2(n) = O(g(n))```，那么```T(n) = T1(n) + T2(n) = max(O(f(n))```，```O(g(n))) = O(max(f(n)```，```g(n)))```。

嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

如果T1(n)=O(f(n))，T2(n)=O(g(n))，那么```T(n) = T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n))```。

```js
function cal(n) {
  var ret = 0;
  var i = 1;
  for (; i < n; i++) {
     ret = ret + f(i);
   }
}
function f(n) {
  var sum = 0;
  var i = 1;
  for (; i < n; i++) {
    sum = sum + i; 
  }
  return sum; 
}
```

单独看```cal()```函数。假设```f()```只是一个普通的操作，那第```4~6```行的时间复杂度就是，```T1(n) = O(n)```。但```f()```函数本身不是一个简单的操作，它的时间复杂度是```T2(n) = O(n)```，所以，整个```cal()```函数的时间复杂度就是```T(n) = T1(n) * T2(n) = O(n*n) = O(n2)```。


多项式量级:```O(1)```，```O(logn)```，```O(nlogn)```，```O(m + n)```，```O(m * n)```。

非多项式量级:```O(2n)```，```O(n!)```。

当数据规模```n```越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增⻓。所以，非多项式时间 复杂度的算法其实是非常低效的算法。

### 1. O(1)

```O(1)```是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有3行，它的时间复杂度也是```O(1)```，而不是```O(3)```。

```js
var i = 8;
var j = 6;
var sum = i + j;
```

### 2. O(logn)

对数阶时间复杂度非常常⻅，同时也是最难分析的一种时间复杂度。

```js
i = 1;
while( i <= n) {
  i = i * 2;
}
```

变量i的值从```1```开始取，每循环一次就乘以```2```。当大于```n```时，循环结束。实际上，变量i的取值就是一个等比数列。只要知道```x```值是多少，就知道这行代码执行的次数了。```x = log2n```，所以，这段代码的时间复杂度就是```O(log2n)```。

```js
i = 1;
while(i <= n) {
  i = i * 3;
}
```

实际上，不管是以``2``为底、以```3```为底，还是以```10```为底，可以把所有对数阶的时间复杂度都记为```O(logn)```。对数之间是可以互相转换的，```log3n```就等于```log32 * log2n```，所以```O(log3n) = O(C * log2n)```，其中```C = log32```是一个常量。 基于我们前面的一个理论，在采用大```O```标记复杂度的时候，可以忽略系数，即```O(Cf(n)) = O(f(n))```。所以，```O(log2n)```就等于```O(log3n)```。因此，在对数阶时间复杂度的表示方法里，忽略对数的```底```，统一表示为```O(logn)```。

### 3. O(nlogn)

如果一段代码的时间复杂度 是```O(logn)```，循环执行```n```遍，时间复杂度就是```O(nlogn)```了。而且，```O(nlogn)```也是一种非常常⻅的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(nlogn)。

### 4. O(m+n)、O(m*n)

```js
function cal(m, n) { 
  var sum_1 = 0;
  var i = 1;
  for (; i < m; i++) {
    sum_1 = sum_1 + i;
  }
  var sum_2 = 0;
  var j = 1;
  for (; j < n; j++) {
    sum_2 = sum_2 + j;
  }
  return sum_1 + sum_2;
}
```

m和n是表示两个数据规模。无法事先评估m和n谁的量级大，所以在表示复杂度的时候，就不能简单省略掉其中一个。所以，上面代码的时间复杂度就是```O(m + n)```。

针对这种情况，要将加法规则改为```T1(m) + T2(n) = O(f(m) + g(n))```。乘法法则继续:```T1(m) * T2(n) = O(f(m) * f(n))```。

## 4. 空间复杂的

空间复杂度全称就是渐进空间复杂度(```asymptotic space complexity```)，表示算法的存储空间与数据规模之间的增⻓关系。

```js
function print(n) {
  int i = 0;
  var a = new Array(n); 
  for (i; i < n; i++) {
    a[i] = i * i; 
  }
  for (i = n - 1; i >= 0; i--) { 
    console.log(a[i]);
  } 
}
```

跟时间复杂度分析一样，第```2```行代码中申请了一个空间存储变量i，但是它是常量阶的，跟数据规模```n```没有关系，所以可以忽略。第3行申请了一个大小为```n```的数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是```O(n)```。

常⻅的空间复杂度就是```O(1)```、```O(n)```、```O(n2)```，像```O(logn)```、```O(nlogn)```这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。

常⻅的复杂度并不多，从低阶到高阶有```O(1)```、```O(logn)```、```O(n)```、```O(nlogn)```、```O(n2)```。

最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。

比如下面这段代码，复杂度是可能是```O(1)```也可能是```O(n)```，就是最好和最坏。

```js
// n表示数组array的⻓度
function find(array, n, x) {
  var i = 0;
  var pos = -1;
  for (; i < n; i++) {
    if (array[i] == x) {
      pos = i;
      break;
    }
  }
  return pos; 
}
```

最好和最坏对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，需要引入平均情况时间复杂度。

要查找的变量```x```在数组中的位置，有```n + 1```种情况，在数组的```0 ~ n-1```位置中和不在数组中。把每种情况下，查找需要遍历的元素个数累加起来，然后再除以```n + 1```，就可以得到需要遍历的元素个数的平均值。

要查找的变量x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解， 假设在数组中与不在数组中的概率都为```1 / 2```。另外，要查找的数据出现在```0 ~ n - 1```这```n```个位置的概率也是一样的，为```1 / n```。所以，根据概率乘法法则，要查找的数据出现在```0 ~ n - 1```中任意位置的概率就是```1 / (2n)```。

```js
1 * 1 / 2n + 2 * 1 / 2n + 3 * 1 / 2n + 4 * 1 / 2n + ... + n * 1 / 2n = (3n + 1) / 4
```

这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。用大```O```表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度是```O(n)```。

在大多数情况下，并不需要区分最好、最 坏、平均情况时间复杂度三种情况。使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，才会使用这三种复杂度表示法来区分。

均摊时间复杂度，听起来跟平均时间复杂度有点像，这两个概念确实非常容易弄混。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。

```js
// array表示一个⻓度为n的数组 
// 代码中的array.length就等于n 
var array = new Array(n); 
var count = 0;
function insert(val) {
  if (count === array.length) {
    int sum = 0;
    for (int i = 0; i < array.length; i++) {
      sum = sum + array[i];
    }
    array[0] = sum;
    count = 1; 
  }
  array[count] = val;
  count++; 
}
```

这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的```count == array.length```时，用```for```循环遍历数组求和，并清空数组，将求和之后的```sum```值放到数组的第一个位置，然后再将新的数据 插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。

假设数组的⻓度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是```O(1)```。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是O(n)。而且，这```n + 1```种情况发生的概率一样，都是```1 / (n + 1)```。根据加权平均的计算方法求得的平均时间复杂度就是```O(1)```。

这个例子里的平均复杂度分析其实并 不需要这么复杂，不需要引入概率论的知识。先来对比一下这个```insert()```的例子和前面那个```find()```的例子，就会发现这两者有很大差别。

首先，```find()```函数在极端情况下，复杂度才为```O(1)```。但```insert()```在大部分情况下，时间复杂度都为```O(1)```。只有个别情况下，复杂度才比较高，为```O(n)```。这是```insert()```第一个区别于```find()的```地方。

对于```insert()```函数来说，```O(1)```时间复杂度的插入和```O(n)```时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个```O(n)```插入之后，紧跟着```n - 1```个```O(1)```的插入操作，循环往复。
所以，针对这样一种特殊场景的复杂度分析，并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。

针对这种特殊的场景，引入了一种更加简单的分析方法: 摊还分析法，均摊时间复杂度。

继续看在数组中插入数据的例子。每一次```O(n)```的插入操作，都会跟着```n - 1```次```O(1)```的插入操作，所以把耗时多的那次操作均摊到接下来的```n - 1```次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是```O(1)```。这就是均摊分析的大致思路。

均摊时间复杂度和摊还分析应用场景比较特殊，所以并不会经常用到。

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。

尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，没必要花太多精力去区分它们。最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。

## 5. 数组

数组(Array)是一种线性表数据结 构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。

第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性:“随机访问”。但有利 就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做 大量的数据搬移工作。

拿一个⻓度为10的int类型的数组int[] a = new int[10]来举例。计算机给数组a[10]，分配了一块连续内 存空间1000~1039，其中，内存块的首地址为base_address = 1000。

计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址:

```js
a[i]_address = base_address + i * data_type_size
```

其中data_type_size表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是int类型数据，所以data_type_size就 为4个字节。这个公式非常简单，我就不多做解释了。

数组是适合查找操作，但是查找的时间复杂度并不为O(1)。即便是排好序的数组，用二分查找，时间复杂度也是O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。

数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效，为了避免大规模的数据搬移，还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。

跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就
不连续了，如果删除数组末尾的数据，则最好情况时间复杂度为O(1);如果删除开头的数据，则最坏情况时间复杂度为
O(n);平均情况时间复杂度也为O(n)。

实际上，在某些特殊场景下，并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多。可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数 据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删 除操作导致的数据搬移。如果你了解JVM，你会发现，这不就是JVM标记清除垃圾回收算法的核心思想吗。

## 6. 链表

相比数组，链表是一种稍微复杂一点的数据结构。先来看，这两者有什么区别。

数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申 请一个100MB大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于100MB，仍然会申请失败。

链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果申请的是100MB大小的链表，根本不会有问题。

链表结构五花八⻔，最常⻅的链表结构，是:单链表、双向链表和循环链表。

### 1. 单链表

链表通过指针将一组零散的内存块串联在一起。其中，把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。把这个记录下个结点地址的指针叫作后继指针next。

其中有两个结点是比较特殊的，分别是第一个结点和最后一个结点。习惯把第一个结点叫作头结点，把最后一个结点叫作尾结点。头结点用来记录链表的基地址。有了它就可以遍历得到整条链表。而尾结点特殊的地方是指针指向一个空地址NULL，表示这是链表上最后一个节点。

与数组一样，链表也支持数据的查找、插入和删除操作。

在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以在链表中插入和删除一个数据是非常快速的。

但是有利就有弊，链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。

可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以希望知道排在第k位的人是谁的时候，需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要O(n)的时间复杂度。

当然删除之前是需要找到对应元素的，也就是需要查询，所以对于删除整体来说复杂度仍然是O(n).

### 2. 循环链表

循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点 指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该 可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。

和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。

### 3. 双向链表

单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。

双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。

结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。

对于删除结点中“值等于某个给定值”的结点，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过前面讲的指针操作将其删除。

尽管单纯的删除操作时间复杂度是O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)。根据时间复杂度分析 中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。

对于删除给定指针指向的结点的情况，已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，还是要从头结点开始遍历链表，直到p->next=q，说明p是q的前驱结点。但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内。

同理，如果希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。

除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录 上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数 据。

实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果更加追求代码的执行速度，可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。

如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。

总结一下，对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)来进行优化;而消耗过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。

### 4. 数组和链表

数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。

数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。

这是因为CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义，为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。

对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。

数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足(out of memory)”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷⻉进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，这也是它与数组最大的区别。

除此之外，如果代码对内存的使用非常苛刻，那数组就更适合。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，导致频繁的GC(Garbage Collection，垃圾回收)。所以，在实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。

### 5. 指针

事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。

有些语言有“指针”的概念，比如C语言;有些语言没有指针，取而代之的是“引用”，比如Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。

将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

### 6. 哨兵

针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行 特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。

哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边界问题”的，不直接 参与业务逻辑。

如果引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。

软件开发中，代码在一些边界或者异常情况下，最容易产生Bug。链表代码也不例外。要实现没有Bug的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。

如果链表为空时，代码是否能正常工作?

如果链表只包含一个结点时，代码是否能正常工作?

如果链表只包含两个结点时，代码是否能正常工作?

代码逻辑在处理头结点和尾结点的时候，是否能正常工作?

### 7. 练习

单链表反转

链表中环的检测

两个有序的链表合并

删除链表倒数第n个结点

求链表的中间结点

## 7.

## 8.


## 9.


## 10. 

## 11. 


## 12.

## 13.


## 14.

## 2. 栈

栈是基础的数据结构，是一种遵从后进先出原则的有序集合。只能从栈顶添加或者移除。通过原生```js```实现一个栈。

```js
class Stack {
  constructor () {
    // 存储栈的数据
    this.data = {}
    // 记录栈的数据个数（相当于数组的 length）
    this.count = 0
  }
  // push() 入栈方法
  push (item) {
    // 方式1：数组方法 push 添加，但并不好，额外引入了数组的属性
    // this.data.push(item)
    // 方式2：利用数组长度，性能比push好，但也是数组的能力
    // this.data[this.data.length] = item
    // 方式3：计数方式
    this.data[this.count] = item
    // 入栈后，count 自增
    this.count++
  }
  // pop() 出栈方法
  pop () {
    // 出栈的前提是栈中存在元素，应先行检测
    if (this.isEmpty()) {
      console.log('栈为空！')
      return
    }
    // 移除栈顶数据
    // 方式1：数组方法 pop 移除
    // return this.data.pop()
    // 方式2：计数方式
    const temp = this.data[this.count - 1]
    delete this.data[--this.count]
    return temp
  }
  // isEmpty() 检测栈是否为空
  isEmpty () {
    return this.count === 0
  }
  // top() 用于获取栈顶值
  top () {
    if (this.isEmpty()) {
      console.log('栈为空！')
      return
    }
    return this.data[this.count - 1]
  }
  // size() 获取元素个数
  size () {
    return this.count
  }
  // clear() 清空栈
  clear () {
    this.data = []
    this.count = 0
  }
}

const s = new Stack()
s.push('a')
s.push('b')
s.push('c')
```

这里栈的```data```和```count```应该是确保不会被修改的，一旦被修改数据就乱了，可以将属性设置为私有，让外部无法访问。

### 1. 包含min函数的栈

就是通过